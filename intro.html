
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Welcome to Functional Pytorch &#8212; Functional Pytorch</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="1. Functions" href="functions.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Functional Pytorch</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1 current">
  <a class="reference internal" href="#">
   Welcome to Functional Pytorch
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Deep Learning Notebooks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="functions.html">
   1. Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="images.html">
   2. Images
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="filters.html">
   3. Filters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gradients.html">
   4. Gradients
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="classifiers.html">
   5. Classifiers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="encoders.html">
   6. Encoders
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="generators.html">
   7. Generators
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="randomwalk.html">
   8. Random Walk
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="automata.html">
   9. Automata
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/intro.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="welcome-to-functional-pytorch">
<h1>Welcome to Functional Pytorch<a class="headerlink" href="#welcome-to-functional-pytorch" title="Permalink to this headline">¶</a></h1>
<p>Introduction to Deep Learning</p>
<p>Welcome, I’m glad you could make it. It took a lot for you to be here and we will get to that but there is a lot we need to discuss so let’s go ahead and get started.  A lot is happening in the world right now there have been some major changes in the last 5 Years in the areas of mathematics computer science psychology and artificial intelligence. Questions Humanity has been asking for thousands of years  Now find a new home in this modern academic discipline known as deep learning.  This book aims to introduce readers to the general concepts and techniques found within this new research area.</p>
<p>to see what is so different about this new computer Revolution we must examine some of the problems that were recently impossible and now are becoming easy to do. personally one of the most surprising examples is that in 2007 according to Marvin Minsky if you took a collection of photographs of cats and a collection of photographs of dogs and you shuffle them up there was not a computer on the planet at the time they could accurately sort them out. This is quite surprising given that a sharp two-year-old would likely be able to complete the task without making any errors.  since around 2012 the world has seen a radical Paradigm Shift both in terms of thinking end in the Practical results delivered by Deep learning neural networks. prior to 2012 other mathematical techniques were routinely outperforming neural networks.  this is now no longer the case deep learning neural networks now dominate in the areas of speech recognition natural language understanding and translation, computer vision,  and many more areas.  the goal of this work is to guide readers through some of the historical developments that made this point-of-view possible and some of the mathematical techniques that make this practical.</p>
<p>our ultimate goal is to understand the workings of the human brain and how such a device can instantiate Minds. Is of the author’s opinion that the tools techniques and languages developed in the areas of computer science signal processing and artificial intelligence have as much to bear on the questions of how the brain and the mind operate as Anatomy physiology psychophysics and so on.  Richard Feynman’s famous words on his blackboard were “What I cannot create, I do not understand”.</p>
<p>Taking this a step further Turing Award winner Don knuth has said “science is what we can explain to a computer, art is everything else we do.”  It can be assumed that such a statement cannot be made without controversy. I have said myself, “if in this decade, if you are not using a computer, you are probably not doing science.” I mean it is hard to imagine a modern scientific study that does not at some point use a computer for data collection, analysis, visualization, or communication. I can imagine that a lot of the readers especially older ones would say oh nonsense but I think that is precisely the problem as a culture we are not being honest with ourselves and we are certainly not communicating to the next generation how important and revolutionary computers have been to progress in science and technology.</p>
<p>“Computation underlies everything we see in biology”- Chris Voigt</p>
<p>Bank of America is one of the largest banks in the country and it might not be surprising when you find out that they installed an automatic check reading system in 1967.  there are numerous books and documentaries about the mega-corporation Monsanto it might be wondering how could it company grow that size and make so much money again it might fit more into perspective when you learn that Monsanto purchased the second computer from Remington Rand the first being purchased by the IRS.</p>
<p>this book will often take an unconventional and somewhat controversial viewpoint in that it is of the author’s opinion (which is not unique) that computers represent more than tools, they are the star of the show.  computation represents a new framework within which Humanity can build the next generation of understanding.  we at we all now have access to a vocabulary that was very rare only 30 years ago.  it can be safely assume that most readers of this text will be familiar with concepts of network, gigabyte, data storage, compression, filters etc.  not many decades ago only students of computer science and University would be familiar with such terms. this provides Humanity with literally a new language with which to discuss the ideas of the Mind And the operations of the brain. Only time will tell but it is ventured that the future of brain science lies in the hands of the computer scientists.</p>
<p>In Alan Turing’s original work he presents a new mathematical object that is more powerful than the field or the vector or the derivative, of the computer program unfortunately it has taken many decades for mathematicians and scientists to agree that programs are mathematical objects.</p>
<p>It is interesting to remark that the early figures in computer science such as Babbage, Turing, Von Neumann, etc.  none of these figures had backgrounds in computer science how could they this subject did not exist yet so I think for young students it is worth noting that some of the best computer scientists of all time were classically trained as mathematicians and natural scientists. Turning Award winner Alan Kay has suggested that students study Natural Science as undergraduates and then study computer science as graduate students unfortunately the nature of the admissions systems prohibit this from being practical.</p>
<p>the history of artificial intelligence is very rich and could not possibly be fully documented here instead an account will be given of some of the lesser-known characters in the computer science world that have given rise to the unique perspective that is given by neural networks.</p>
<p>I think there was an unfortunate and largely unrecognized split in the history of academic computer science programs and didn’t might have evolved largely around prohibitive costs of  early computer systems This unfortunate event was of course the creation of computer science departments, short-sighted thinking and high costs and difficulty of used have required that computers had been thought of as their own independent academic discipline removed from Natural Science, mathematics, philosophy, and language.  this seems to have had a particularly strong impact on the biological sciences especially neuroscience it would seem that the one group that needs to be thinking about computation the most largely ignores the topic.   This might simply be an artifact of the expensive nature of computers and the expensive nature of biological specimens I can imagine some sort of scenario with department heads fighting over budget and the decision has to be made whether or not to buy lab rats or to buy workstation computers,  because when I look at academics labs today I don’t see very many that have both.  somewhere along the line we got this unfortunate idea that if you’re going to do natural science you don’t really need to bother with computer programming and if you’re going to do computer program and you don’t need to bother with natural science. This might be what led Richard Feynman to say During a lecture given to Bell Laboratories, “ I don’t believe in computer science”.  Alan Kay shares a similar lack of excitement for software engineering.</p>
<p>I think it is very hard to appreciate how new software is even the idea of it.  Aida Agustin Countess of Lovelace was likely to be the first human to really see the true potential of computing machinery, but she was a century ahead of her time at least and it would seem even few today appreciate how far out her ideas were.  I recently came across an article on Edsger Dijkstra (also Turing Award winner), in which it is reported that in 1957 when he applied with the local government for a marriage license They requested that he list his occupation which he wrote “Programmer”  which was unacceptable to the authorities there being no such job at the time in the Netherlands.</p>
<p>The very concept of software  is only a few decades old and most scientific fields and academic disciplines have only superficially embraced the digital Revolution.  it seems likely that within a few decades most of the science that is considered solid today will look closer to something like Alchemy when the subject is reexamined in a computational in machine learning framework.</p>
<p>this sounds like such a harsh statement what could that possibly mean.  it is projected now that in medicine total accumulated knowledge is doubling on the order of every two years,  by the mid-2020s it is projected to double every two months.  what does that mean? What does that mean for science? What does that mean for scientists? The old notion that we will get the best and the brightest and they will work really hard and study all they can to learn everything there is to know just,  simply isn’t going to work.  the scales of the problem are changing and we need to change the scales of our solution  and to do that we need to change the scale of our thinking</p>
<p>The situation is no different in law, finance, media,  and of course the Natural Sciences such as chemistry, physics, biology.</p>
<p>today’s students in school learn how to do math problems one at a time,  a modest personal computer with a graphics card that any child would have for video games can now routinely perform in excess of a trillion math problems every second.</p>
<p>Can you imagine a course on the formal theory of computation as being a prerequisite for a neuroscience class? how many electrophysiologists have ever even heard of a memristor? Those that study the brain still use the terms that were invented hundreds of years ago by individuals looking through homemade microscopes.</p>
<p>“Enormous disparity with what you could do with computers and what most people did” -Alan Kay</p>
<p>We want to understand the brain but let’s first consider some of these new deep learning Technologies and how they can take images from cameras and turn them into decisions.
The first thing we have to consider is that modern cameras record millions of samples.  the smart phone in your pocket is likely to have at least an 8 megapixel sensor. That’s 8 million numbers that are recorded to your phone’s memory every time you take a picture each number represents the brightness and color of each pixel.  now typically we want to make the decision from that photograph such as whether or not the person has their eyes closed then the camera can wait to take the picture.  we can’t think of eyes being open or closed as a single binary number 0 if they’re closed 1 if they’re open.  If the camera software wants to be able to decide if eyes are open or closed it has to look at those 8 million numbers and reduce them to a single binary digit or bit.  so that’s the real question how do we take a list of 8 million numbers and recombined those numbers into a single bit that represents whether or not people in that scene have their eyes open or closed. This is what deep learning  makes possible reducing 8 million numbers to just one.</p>
<p>For most computer vision tasks it is simply not practical for humans to write out logical rules  to say drive a car or recognize handwriting.  these are skills and knowledge sets that we embody but we do not have explicit access to them that would make it amenable to writing out computer programs.</p>
<p>we do not teach babies how to walk I explaining to them the dynamics of their ankle muscles. What are she actually do when we are walking is something that’s not consciously available to us,  moreover if you think about your toes too much you’re liable to fall down the stairs.</p>
<p>the same is true of language we think speak end listening terms of ideas and it’s difficult to hear the sounds that make up the words we use.  if you repeat a single word over and over again you can begin to hear the sound and it sounds like you’re making strange noises that have no relation to the real world.</p>
<p>to create computer systems that have human-like Vision capabilities  requires a different approach to computer science.</p>
<p>The amazing and encouraging news about deep learning is that it is vastly simpler than the traditional methods of programming that create mountains of code and kludge.</p>
<p>The windows operating system is reported to have 50 million lines of code. As Alan Kay says Is it that complex or did we just make it complicated.  What sort of functionality does Windows provide to warrant that?</p>
<p>with advances in Virtual machines I think we might soon see the end of the operating system as we know it now and instead we will see the rise of a new ecosystem of deep learning agents based around microservices and messaging.</p>
<p>Extremely powerful deep learning systems such as those that can sort out photographs into one of 1000 categories with a higher accuracy than humans requires many orders of magnitude less code than something like Microsoft Office. In some frameworks completely specifying a deep learning system takes only 10 lines of code.</p>
<p>we are going to see an incredible shift in what computers are capable of.  this is going to require a radical retooling of our education system to emphasize this new style of computer science it’s often much better to study mathematics and physics  then to take the traditional computer programming classes offered by most departments.</p>
<p>This Modern deep learning Frameworks very quickly the limiting factor is going to become creativity.  I can imagine this is how people must have felt with the Advent of digital computers compared to the slow laborious process of creating custom analog circuits.  or maybe with the Advent of things like Fortran during the days when computers were programmed in absolute binary.</p>
<p>I myself got to witness the rise of the GUI having been born in 1985 and being exposed to computers early I was able to witness windows 3.11 in 1993 and very quickly became familiar with all of its features.  It did not occur to me at the time that most adult computer experts we’re not as familiar with the GUI as I was.</p>
<p>I’m sure many of the older programmers we’re shocked and dismayed at how literally children were able to skillfully execute Computer Applications that could have only been dreamed of just 10 years before.</p>
<p>the only real hindrance  Two computer science and artificial intelligence in this era is that we simply do not teach it.  what I mean by that is that we do not teach it from the beginning from the history from the science.  instead we teach children how to use applications, how to run an interface,  we teach them the GUI. We teach children how to type rather than explain speech recognition technology.</p>
<p>It would seem that most computer programs in the world are elaborate simulations of filing cabinets and paper offices of yesteryear.</p>
<p>the real excitement in computation is viewing it as a natural science as a branch of physics and Mathematics.</p>
<p>We teach children to manipulate numerals on paper to compute sums and products when they have no concept of the almost unimaginable progress in Computing Machinery in the last century.</p>
<p>One of the first reliable mechanical calculator was produced in the 1890s by an 18 year old student,  so no one reading this can make the excuse that calculating technology did not exist when you were learning mathematics.</p>
<p>We do students a terrible disservice by claiming that mathematics is different than Computation is different than physics is different than chemistry and so on.  I would argue that almost all of the major breakthroughs that we will see in the next few decades will Arise at the overlap and the intersection between traditional academic subjects with computation playing a central role.</p>
<p>it has been claimed that computer vision is AI hard which is a term adopted from complexity Theory meant to suggest that computer vision is as hard as any problem in Ai and if we can successfully solve computer vision problems then we will be able to transfer those solutions to other AI domains.</p>
<p>the good news is that you already understand most of the ideas needed for advanced AI for example a self-driving car.   30 years ago this would not have been the case but now everyone reading this has some understanding of digital cameras, pixels, computer memory,  filesize, etc.</p>
<p>the new vocabulary which has developed from the commercialization and economic success of consumer electronics and personal computers  provides a Rosetta Stone for the concepts of deep learning.</p>
<p>we had all taking pictures with a smartphone and now even the smallest child is familiar with the concept of an image filtering, cropping, resizing.  this is how we are going to approach deep learning as a series of filter and resize operations.</p>
<p>Here we see some examples of image filters
Figure
edge detection</p>
<p>Figure
Using pooling to reduce the size of an image</p>
<p>Most of the examples we will focus on creating a self-driving Rover. But we want you to keep in mind that these techniques can apply to almost any problem in computer vision, including problems that don’t seem like they have anything to do with robotics such as problems in medicine, finance, law, forensics, etc.  We will show how we conduct our deep learning self-driving research in the hope that we can grow the deep learning community and make sure such valuable knowledge is not unique to anyone group.  The vehicle we use in our laboratory research is the Brookstone Rover 2.0. We connect to the Rovers on board WiFi and assume control of the motors and the camera.</p>
<p>The controls are simplified down to three actions left right and forward.  the end goal is for the system to take in images from the camera and then automatically decide if the rover should turn left right or forward.  When this happens many times a second the rover can then successfully navigate a scale road course on its own.</p>
<p>we have humans Drive the Rover around the track looking through the camera frames  choosing the appropriate action left right or forward.  this allows us to create a large table set example images together with the action chosen by a human.</p>
<p>This is known as creating a policy network using supervised learning. The human drivers are the ones providing the Supervision in the form of the dataset with labeled frames.  the network then acts as a policy which choose an action given an input image.</p>
<p>this is not unlike Auto tagging in Facebook when you upload an image to Facebook it tries to decide who you are but it first had to have encountered photographs that were labeled as having you present,  these initial labels provide the supervision that can then be generalized so that new photographs can be recognized as you even though you might have a new haircut and so on.</p>
<p>as we have said this was impossible in 2007 what does it mean for the planet when such a difficult Problem moves from the Impossible category to the not that difficult category.</p>
<p>the general problem of deciding which direction to turn the steering wheel when given the input from a dash cam is no different than finding a cancerous tumor in a brain scan.</p>
<p>how many medical schools are teaching these ideas? I am worried that the next generation of Physicians by the time they go through their twelve or so years of schooling and accreditation will find themselves experts in techniques that will feel more like leeches and bloodletting than  state-of-the-art medicine in the 2020s and 2030s.
In terms of Education there is good and bad news.  the bad news is that most of it school programs offer very little in terms of computer programming, even worse students have no concept of the history of computers what they are made of how we built them first out of paper and out of metal than out of sand. the good news is that’s the future of intelligent machinery does not lie in programming as much as it lies in what we now call machine learning techniques and machine learning is really a branch of algebra which most school children are exposed to ad nauseam.</p>
<p>The familiar y = mx + b  from high school algebra now takes the center stage as the core component in a modern deep Learning System. We remember that if we use this formula as a recipe to construct a table we can create a graph of the function and visualize it below:</p>
<p>we can see that this formula this recipe has created a line we can now think of that line as a decision boundary on one side of the line we will put all the yes on the other side we will put all the no on the other, an example of yes or no maybe photographs of a skin lesion or the yes would mean there’s a good chance this is a photograph of skin cancer and they know would mean this is not something to worry about.</p>
<p>Real world data examples can rarely be separated with a single equation the power of modern deep Learning Systems is the ability to combine thousands of these equations together to create a very complex decision space. it would be impossible for a human mind to fine-tune all of these lines that create the decision space. instead we let the data itself tune our computer system. We start with an initially random set of decisions and the data adjusts the decision boundaries as the network learns.</p>
<p>How do children learn shapes and colors? how do children learn the letters of the alphabet? do you remember your training in cursive handwriting? We consider these tasks child’s play but really they are at the heart of what it is that our brain can do. there is a famous saying in artificial intelligence that the hard problems are easy and the easy problems are hard. handwriting recognition is one of those problems that was much harder than people ever realized until they try to build a machine that could reliably recognize unreliably written handwriting.</p>
<p>Handwriting recognition is one of the most well study problems in artificial intelligence Vision research. The famous MNIST Dataset  is arguably one of the most studied it sets in neural network research.  it consists of many thousands of digitized handwriting samples of numerals 0 through 9.  we can think of this as an image classification task of photographs of a number of individuals here the numbers is 0 through 9 end reminiscent of a game on Sesame Street we now have to name the numeral in the photographs as we encounter them.</p>
<p>it is rather difficult to imagine that this problem is hard we forget being so young and we learned this problem so long ago. part of the situation is that most of our brain is dedicated to this sort of tasks and it has become in so ingrained in our daily function in modern society  that most of it take it completely for granted that we can do this in unknown and noisy environments.</p>
<p>example of 2 digits where we can see the pixels where the image has been digitized.  initially the signal was analog ink across paper most likely with a ballpoint pen  which was then digitized by measuring the overall level of ink in each Square when the grid graph paper was superimpose on the original analog image.  we now have a digital signal that we can use as input to a digital computer program.  if we look at these two digits now as one giant number which is closer to how the computer Would understand these two images we can see that it is difficult to see that these would be the same digit.</p>
<p>we have mentioned before that the goal is to reduce all of these squares to a single number.  we can think of that number as being the answer to an unambiguous question such as is this the number to 0 if it is not 1 if it is.  this is essentially the same problem as trying to classify all of the categories but we can start with this one for simplicity.</p>
<p>Are some of the mathematical recipes that we could use to reduce all of these numbers to a single-digit which might contain the answer to our question?  one thing we might want to try is to just add up all the numbers.  we could also try multiplying all the numbers together.  we could compute the average.  we could pick a single value at random.  you can imagine lots of different ideas.  all of these are examples of hand-tuned features.  pre-processing steps invented by humans as a way of making the problem easier for the computer to understand.</p>
<p>the limiting factor in most computers is not speed but memory.  most experiments in artificial intelligence are really a compromise  in problem size and computer memory available.  during one said let the outside world consists of an independent paper tape.  unfortunately most research groups do not have an infinite supply of paper or computer memory.  thus many techniques have been developed over the years to try to reduce the problem size to something that is practical to run on  a digital computer.</p>
<p>a lot of great research has been developed and we encourage the reader to explore the many ideas people have developed in AI vision research along the way to where we are now they will likely  very valuable in the future</p>
<p>Yann lecun The original developer of convolutional neural networks has tried to explain their unreasonable success I claiming that they make no assumptions,  in that the random connections at the start of the algorithm make no assumptions about the problem at hand.  this agnosticism is that the heart of the power of the deep learning framework.  hand-tuned features have been replaced with pure randomness (or our approximation thereof).</p>
<p>it is very interesting to think about where random is comes from.  this is a very rich scientific topic and could use a book of its own.  we will return to the idea of random this many times randomly throughout this text.</p>
<p>just like in the human brain the neurons in a deep learning neural network are not connected in any particular fashion randomly connects to its neighbors.  Randomness has power in its possibility.  for something to be random means it’s nothing in particular, which strangely enough means it has the possibility to become precisely what you need.</p>
<p>Put another way you are searching for a function in mathematical machine that will solve a particular problem that you might have might be a business problem with science problem a medical problem none the less you are looking for a map in mathematical map  a function that will take in some sort of input and provide you and output such that the output is an answer to your question.</p>
<p>it is a very strange notion to think of a mathematical object as answering a question but really this is at the heart of what artificial intelligence is all about.  in a way it was never about computers it’s about the mathematics it’s about finding a mathematical object that we can then instantiate in the physical world with what we call a computer.</p>
<p>if you remember from high school manipulating primitive functions is not an easy, and often not an enjoyable task.  I still remember in countering sine and cosine for the first time and thinking “well they haven’t told us what to do with letters”.  We had been given rules about how to manipulate numerical symbols it was completely it was clear what the next step should be remembered all the rules. You look at each section of the paper and as long as you remember what type of problem it is that you’re trying to saw you know how to add pencil or eraser to the paper to prove the problem forward one step at a time. This is precisely the model that touring used to build a formal theory of computation.  at the time computers were people computation was an occupation just like bakers bake, computers computed.</p>
<p>Have you ever thought about how a computer calculates something like the sin function? When you encounter something like sin(x) it’s not obvious what to do with that at least not with the rules of arithmetic,  and arithmetic we have a few basic operations +, -, /, *  and we represent the combination of numerical symbols with these operators using something called it infix notation meaning that we put the operation in the middle such as 1 + 1.  we could have instead decided that we would write out the word “add”  and then supplied the numbers to be added after as a list.  this would look like add(1,1)  which of course would result in 2.</p>
<p>Mathematicians like to use single letters for ideas, (This is often very helpful as a reminder that we can manipulate the ideas as if they were symbols an idea the heart of computer programming) thus we can use the letter f for the word add then we have f(1,1) = add(1,1) = 1 + 1.</p>
<p>You might think what what is all the point of this well the point is to remember that  we don’t actually know what our function is that we want to compute,  in another words we don’t know the formula that will answer the question we need answered.</p>
<p>this is very different than most math problems normally we are given a problem and we have to calculate the answer.  with deep learning we know what the answers look like but we don’t know what the right question is and so we set up a random neural network that in some sense can answer all possible questions and then we use the data to adjust that Network until that solves the question we are asking. Once we have the right formula a computer can calculate the solution very quickly.  the power of deep learning is the ability to have a computer find the appropriate formula given only examples of the task to be performed.</p>
<p>most people reading this have no idea how their ankles and Toes work,  what I mean is that for most people if you think to consciously about your ankles and Toes you might fall down the stairs.  and the same way we don’t really understand how speech works not any personal level it is very difficult to think about the noises you make and the ideas at the same time.  as no doubt you have done before try taking a random word and repeating it over and over again until you can start to hear the sounds when you can finally hear the sound you are making it most likely no longer makes any sense as an idea,  instead of dolphin sounds like a strange an abstract  and surprisingly complex set of noises.  if we could hear the language of our eardrums it would not make any sense to us.  if we could see what fell onto our retina we could not make sense of it.</p>
<p>Our Conscious experience lies at the very top of a deep learning style hierarchy in that we deal with the world at the level of symbols, ideas, concepts, and emotions.  it took humans thousands of years  to develop the linguistic and scientific tools that enable us to view the world on multiple scales.  humans have known for thousands of years that we see the world not as it is but as we are.  it has been jokingly said that until I believe it I cannot see it.  much work needs to be done exploring how deep learning tools can help us better understand perceptual and cognitive bias.</p>
<p>A child born in France will learn French and a child born in Spain will learn Spanish.  the human brain is also endowed with this randomness the seed of possibility.  it is not in our DNA the instructions to build a machine that can understand French or Spanish there’s too much information  for that system to manage.  Instead the DNA and codes for a semi general-purpose learning machine.  semi in the sense that I lot of our neural structure and large-scale architecture has evolved for the processing of auditory signals and the creation of verbal response.  general purpose in the sense that the particular settings of the network the weights between neurons the connections between neighboring brain cells is not completely specified at Birth.  instead it is set up a random initial condition and then the brain dynamically evolves through development to connect the brain cells together that are solving problems.</p>
<p>Since the age of Darwin we have understood that biology is dominated by the concept of an ecosystem and competition,  the so-called survival of the fittest.  the living Jungle of brain cells we all carry can be thought of as a microcosm of evolution on a much spit up time scale.</p>
<p>it is important to remember that the metaphor are not exactly the same but the general concepts are very similar. Each brain cell can be thought of as a small animal.  it has no idea it’s a brain cell.  how could it possibly know such a concept for it itself does not have a brain of its own.  it does however have a very powerful form of intelligence encoded as a dynamic protein Network.  the interactions between the DNA in a cell and the protein networks inside the cell is extraordinary really complex and not entirely well understood.  It is very exciting to think about how artificial intelligence will be able to catalog and a say all of the components and processes that occur in all of the thousands of different cells throughout our body.</p>
<p>this tiny brain cell a miniature protein computer is embedded in an ecosystem of many similar and many dissimilar miniature protein computer cells.  these cellular computers must all work together in a dynamic cooperative and competitive fashion to ensure that they receive nutrients and information.</p>
<p>it is very interesting to think of something as requiring information.  can you imagine depriving a child of sources of information? no story books no TV no toys.  we would consider that torture.  human beings need information streams at the brain level.  it is very interesting to think about supplying these cellular computers, these brain cells with information.</p>
<p>without a proper inputstream neurons will atrophy and die.  we all know that crossword puzzles can help with dementia the reason why is because we are simply activating neurons that might not have been asked to do anything in a very long time.  are mental Pathways literally gets stuck in a rut.  without novel stimulus is difficult to get our brain cells active.  It is important that we curate streams of information to keep the mind healthy.</p>
<p>In the forest animals have to compete for places to live and food to eat in the brain neurons have to compete To represent ideas and memories. In the first letter of this sentence we see capitalize as a vertical bar you have neurons in your brain that are identifying this visual feature.  in the context of the other visual features you are able to conclude that that is the word in and you were able to use that knowledge and understanding the rest of the sentence.  thinking about thinking is a very funny business and learning about learning is no different.</p>
<p>In your brain the neurons that represent this vertical bar had to compete with other brain cells for the ability to represent this feature.  in ecology speak out of finding a niche in particular Corner in the ecosystem in which an animal can carve in existence.  we can think of a squirrel in a mature forest that relies on the acorns and the trees for its survival has found and established  a place and a manner in which to live. In the same way we can think of the neuron that represents the vertical bar as having found something to do the job which has utility to the local neurons as well as the brain and the organism itself.  phone are on now performing a useful function is allocated resources that will enable it to survive as a protein computer.</p>
<p>Neither the squirrel nor the oak tree made a conscious decision to enter into their arrangement rather this is something that the forest ecosystem evolved into over an extremely long time scale. the same is true for many of the operations in the brain there is an initial collection of animals that form in ecosystem and they must evolve over the course of seconds, minutes, days, and years into a mature healthy brain.</p>
<p>This is now uncontroversial that there are the cells in your brain there are neurons firing right now in response to the vertical lines in these letters this is how your brain is able to understand the letters the words and ultimately the ideas.  we pay attention to the ideas level.  you cannot pay attention to the stripes on the Tiger in fact you don’t even want to think about the tiger too much you need to think about running away.</p>
<p>most of our capabilities the result of culture software so to speak.  are basic or set of values and beliefs can be thought of as an operating system that rides on top of our basic perceptual system.  much of the difficulty in understanding the complexities of human behavior comes from the near and possibility of separating something that our brain and do from something that our culture can do.</p>
<p>it is inconceivable of thinking of raising a single human in isolation  as we have discussed humans need information and not just statistical information we need meaningful information information that can help us train our neural networks, without this steady stream of information life is agonizing and inhumane.</p>
<p>the few unfortunate children that have been found in a feral state exhibit a few of the behaviors typically seen in modern society, Rehabilitation in terms of learning how to read or even speak is off an extremely difficult for these children.  this would seem to be evidence that the ecosystem of brain cells evolved quickly when young and while it is possible to create neurons throughout adulthood once major routes are established those are the routes that be reinforced</p>
<p>Little research has been done on the idea of developing a robot culture,  end machine culture that hopefully shares many of the values of human cultures.  it is in very important that we build machines I can understand what is important to us there are the famous thought experiments in which you ask a machine to make some more paper clips and so it melts the planet to get all the Battle of can the create as many paper clips as it can.  it is the story as old as time that if the genie lamp or the monkey call the magical Talisman that can grant 3 wishes but inevitably we never get what we ask for because we were not careful enough to specify precisely what it is that we cared about when we made our request.  I think this is a serious issue but not one that will hold back any practical developments in artificial intelligence.</p>
<p>one of the remarkable results of the recent computer Triumph in the go board game tournament was demonstrating the need for a value function.  typically we praise computers in their ability to predict precisely what is going to happen give it a particular sequence of actions.  with simple games this works well but with situations like the go board game the space of possible actions grows too fast  end it becomes extremely difficult to predict what’s going to happen. Instead the computer scientist had to create something with which we are all very familiar a gut feeling.  a simple response that in itself carries very little information something more like the plus or minus grades from elementary school.  the signal does not carry that much information but what it does tell us is clear does this feel good or does this feel bad?  this simple ships I’m trying to predict what will happen to instead trying to decide how does the current situation make me feel this was the major breakthrough that allowed the artificial intelligence to win at the game go.</p>
<p>this sounds rather poetic about the computer with feelings it is really much more abstract and involves a mathematical function to calculate “the value” of a situation.  while the math is abstract I think it’s important to consider that they understand how to accomplish a task like this the value of each position.  with something more familiar like chess we can think of what is the value of having a queen unprotected,  or what is the value of a check.  for the case of the game go the value function was constructed by looking at many millions of example games played online.  imagine the entire board game as a film strip.  in the first frame we see the initial board and then the last frame we see the final board outcome and the winner.  we can Now sample from frames randomly throughout this movie snapshots from during the game and we know who won the game because we can look at the last frame. we can then quiz the computer give it a frame from the middle of the board game film strip and ask it who do you think will win? just teaches the computer how to Value different strategic positions.  by knowing who ultimately wins The computer can learn from each frame of gameplay footage and try to build an understanding of what it is about a particular combination that will ultimately cause that side to win or lose.</p>
<p>many people around the world were absolutely shocked to see that a computer was able to beat the world champion in such a complex game.  in many countries this game is a professional occupation that goes back more than 3,000 years.  any computer scientists thought it would take at least another decade before computers would be able to tackle a task of this complexity.  success of this application lies in the use of a deep neural network together with the concept of a value function.</p>
<p>let me think of human values we don’t really think of board games.  we think of the behaviors that are appropriate for a healthy Society.  it is very important that we start the conversation about how to build a machine culture that has a value system that is closely aligned to the things we care about.</p>
<p>We are now witnessing the creation of a mechanical Kingdom.  just like the emergence of the animal kingdom and the plant kingdom the ecosystem will have to adapt I just new branch of the Tree of Life. We should be very careful to look at how our planet has adapted to drastic changes in the tree of life such as the rise of multicellular life, the Cambrian explosion, or the emergence civilizations.</p>
<p>it is often said that development and artificial intelligence will be more important than developments in electricity or even fire. Part of the purpose of this book is to inform people of the severe emergency that we now face.  the emergency is not that people are working on AI is that not enough people are working on AI. We need students researchers and practitioners in every field to start thinking about these questions to seriously consider what their field is going to look like when what it is they do now end be efficiently automated.</p>
<p>If you do not believe that artificial intelligence is going to be a major Force in the world then you have to ask yourself what is it about nature and Science that is going to change in the next few decades to stop this from happening?  I don’t think this is something we need to be afraid of any more than we should have been afraid of the internet. I can imagine people in 1992 saying I’m worried that if we connect up all of these computers then things will move so fast that all of the businesses and industries as we know them now will be obsolete.  they would have been right in that many Industries have been just dropped it but I don’t think many people view the internet as overall being a bad thing.   countless businesses have been replaced by internet technology  but overall we still view the internet as a very positive force for humanity.</p>
<p>Von Neumann famously said that 4 or 5 computers might saturate the World Market but he was thinking too much about machines and not enough about people he did not realize that human beings would want to use computers to solve more pedestrian problems, to fulfill more human goals. It would be hard to imagine explaining to the early Computing pioneers what most people actually do with their computers today.  that it is not uncommon for school children to have super computers in their pocket but they are not used for numerical calculations but they are used largely to replicate paper Telegraph messages and film cameras.</p>
<p>Isaac asimov said I do not fear computers I fear the lack of them. And Arthur C Clarke said that any computer that can be replaced by a machine should be.</p>
<p>We believe that artificial intelligence theory namely neural network technology should be taught to students of all age as soon as possible.  I can imagine a dangerous situation in which very few companies or countries understand the importance of artificial intelligence. imagine that at app that runs on your telephone can tell you whether or not the mole on your skin is cancer this simple technology could save your life but like most  applications it will be protected with a password the question becomes when life-saving medicine is an application who controls the passwords?</p>
<p>I recently asked some student if they could describe in english the taste of strawberries. After a second or two, one of them blurted out ‘heaven’. Instantly they were forced to use a metaphysical concept. There isn’t really any way to describe experience in english. Alan Kay tell the joke of a famous pianist who after playing a piece is asked a question from the audience, after the question is presented the pianist thinks for a second and then without saying a word sits down and plays the piece again. We cannot talk put music into english anymore than we can explain the taste of strawberries. How can we then as turing suggested teach a computer to “enjoy strawberries and cream”?</p>
<p>When we talk mathematics, we may be discussing a secondary language built on the primary language of the nervous system. - John Von Neumann</p>
<p>This work brings together research from computer science and neuroscience into a framework that elucidates some of the functions of biological brains. It is di cult to describe the brain given that I am a brain and cannot be objective in the matter. It is made even more di cult by the e↵ects of evolution, both genetic and cultural. Many of the capabilities we attribute to humans brains are due to an elaborate mix of genetics, development, information processing, and cultural education. In this work we will focus on the information processing aspects of the brain, i.e how the brain learns to adapt itself to create action policies.</p>
<p>There likely at least many ideas about brains as there are brains in the world. Therefore, it is important to emphasize the historical context of much of the modern artificial intelligence and machine learning achievements that are now commonplace. As LeVar Burton says “you don’t have to take my word for it.” It is a goal of the current document to leave many of the original voices intact, to convey how the story of artificial intelligence is the story of humanity and our journey to understand our place in the universe. Sir Karl Popper Popper writes “Before we as individuals are even conscious of our existence we have been profoundly influenced for a considerable time (since before birth) by our relationship to other individuals who have complicated histories, and are members of a society which has an infinitely more complicated and longer history than they do (and are members of it at a particular time and place in that history); and by the time we are able to make conscious choices we are already making use of categories in a language which has reached a particular degree of de- velopment through the lives of countless generations of human beings before us…We are social creatures to the inmost centre of our being. The notion that one can begin anything at all from scratch, free from the past, or unindebted to others, could not conceivably be more wrong.”[99]</p>
<p>It is hard to find the origins computation and harder still to justify linear narratives that explain the developments of technology, computing technology in particular. A common place to start is in 1844 with Ada Lovelace who pronounced “It does not appear to me that cerebral matter need be more unmanageable to mathematicians than sidereal &amp; planetary matter &amp; movements; if they would but inspect it from the right point of view…I have my hopes, and very distinct ones too, of one day getting cerebral phenomena such that I can put them into mathematical equations–in short, a law or laws for the mutual actions of the molecules of brain. I hope to bequeath to the generations a calculus of the nervous system.”[57]</p>
<p>Nineteen years later in 1863 Samuel echoed “There are few things of which the present generation is more justly proud than of the wonderful improvements which are daily taking place in all sorts of mechanical appliances. It is unnecessary to mention these</p>
<p>here, for they are su ciently obvious; our present business lies with considerations which may somewhat tend to humble our pride and to make us think seriously of the future prospects of the human race. If we revert to the earliest primordial types of mechanical life, to the lever, the wedge, the inclined plane, the screw and the pulley, or (for analogy would lead us one step further) to that one primordial type from which all the mechanical kingdom has been developed, we mean to the lever itself, and if we then examine the machinery of the Great Eastern, we find ourselves almost awestruck at the vast development of the mechanical world, at the gigantic strides with which it has advanced in comparison with the slow progress of the animal and vegetable kingdom. We shall find it impossible to refrain from asking ourselves what the end of this mighty movement is to be.” [22]</p>
<p>John McCarthy the scientist who coined the term artificial intelligence claimed “Well, there are two ways of looking at things. You can either look at it from the point of view of biology, or from the point of view of computer science. From the point of view of biology, you could try to imitate the nervous system insofar as you understood the nervous system, or you could try to imitate human psychology insofar as you understand human psychology. The computer-science way of looking at it says that we look at the world and we try to see what problems it presents in order to achieve goals and think about the world rather than about the biology per se. And I would say that the computer-science approach is the one that so far has had the most success.”[104]</p>
<p>Further, McCarthy defined Artificial Intelligence as a science, namely the study of problem solving and goal achieving processes in complex situations. A basic science like mathematics or physics, requiring experimentation, and with problems distinct from applications, and distinct from the study of how human and animal brains work.[104]</p>
<p>The Internet encyclopedia entry on the so called “AI winter” lists 1970 “as the abandonment of connectionism. The first spring would come in the 1980’s with John Hopfield’s interpretation of computation as physical process, trying to consider digi- tal computers, analog computers, and the brain as having things in common.</p>
<p>This idea was a revival of much older ideas in particular John von Neumann in 1951 describes how “it is perfectly possible that the simplest and only practical way to actually say what constitutes a visual analogy consists in giving a description of the connections of the visual brain. A new, essentially logical, theory is called for in order to understand high-complication automata and, in particular, the central nervous system. It may be, however, that in this process logic will have to undergo a pseudomorphosis to neurology to a much greater extent than the reverse.”[157]</p>
<p>Indeed Konrad Zuse, who independently pioneered many of the early ideas in hard- ware and software design, in a 1980’s lecture at the computer history museum lecture tells the audience “I concentrated my ideas more on the relations between man and machine…..I didn’t see any border between calculating and thinking…surely at that time the computers we could make at that time they were far away from being elec- tronic brains…today I hope that your and my brains are ahead of the computers.”[172]</p>
<p>Butler in 1863 had already asked “In what direction is it tending? What will be its upshot?” [22] As Turing suggests it is the general goal of AI is to “investigate the question as to whether it is possible for machinery to show intelligent behaviour. It is usually assumed without argument that it is not possible.” [151] In line with Turing, it is the authors contention “that machines can be constructed which will simulate the behaviour of the human mind very closely.”[151]</p>
<p>Butler pointed out the connection between Darwin’s ’new’ theory of evolution and the proliferation of mechanical aids that we becoming common in the 19th century. He writes “We regret deeply that our knowledge both of natural history and of machinery is too small to enable us to undertake the gigantic task of classifying machines into the genera and sub-genera, species, varieties and sub-varieties, and so forth, of tracing the connecting links between machines of widely di↵erent characters, of pointing out how subservience to the use of man has played that part among machines which natural selection has performed in the animal and vegetable kingdoms, of pointing out rudimentary organs which exist in some few machines, feebly developed and perfectly useless, yet serving to mark descent from some ancestral type which has either perished or been modified into some new phase of mechanical existence.” [22] Butler drawing on first principles and seeking to generalize the tree of life reveals “as the vegetable kingdom was slowly developed from the mineral, and as in like manner the animal supervened upon the vegetable, so now in these last few ages an entirely new kingdom has sprung up, of which we as yet have only seen what will one day be considered the antediluvian prototypes of the race.”[22] He uses the words “mechanical life,” “the mechanical kingdom,” and “the mechanical world”. Analogous to how a naturalist would describe a turtle shell Butler describes a pocket watch “the beautiful structure of the little animal, watch the intelligent play of the minute members which compose it; yet this little creature is but a development of the cumbrous clocks of the thirteenth century it is no deterioration from them. The day may come when clocks, which certainly at the present day are not diminishing in bulk, may be entirely superseded by the universal use of watches, in which case clocks will become extinct like the earlier saurians, while the watch (whose tendency has for some years been rather to decrease in size than the contrary) will remain the only existing type of an extinct race.”[22]</p>
<p>The idea of a digital computer was already “an old one” in 1950.[152] Turing wrote of Charles Babbage, Lucasian Professor of Mathematics at Cambridge from 1828 to 1839, and how he “planned such a machine, called the Analytical Engine, but it was never completed.”[152] Even though it was never fully operational Babbage’s Engine nonetheless was able to drive the worlds imagination. Harry Wilmot Buxton proclaimed “The marvelous pulp and fibre of a brain had been substituted by brass and iron: he had taught wheelwork to think.” Doron Swade, biographer and historian of computing describes how Babbages remarked “It will jam, it will break, but it will never deceive.” Babbage, tasked with eliminating errors in mathematical tables has been said to exclaim “I wish to God these calculations had been executed by steam!”</p>
<p>Gregory Chaitin relays how Gottfried Wilhelm Leibniz, long before Baggage had “talked about avoiding disputes and he was probably thinking of political disputes and religious disputes by calculating who was right instead of arguing about it! Instead of fighting, you should be able to sit down at a table and say, “Gentleman, let us compute¡‘ What a beautiful fantasy!”[30]</p>
<p>Turing knew that “although Babbage had all the essential ideas, his machine was not at that time such a very attractive prospect. The storage was to be purely mechan- ical, using wheels and cards.”[152] Turing’s most detailed information of “Babbages Analytical Engine comes from a memoir by Lady Lovelace (1842). In it she states, ‘The Analytical Engine has no pretensions to originate anything. It can do whatever we know how to order it to perform”’.[152]</p>
<p>Hartree in 1949 added “This does not imply that it may not be possible to construct electronic equipment which will “think for itself,” or in which, in biological terms, one could set up a conditioned reflex, which would serve as a basis for learning.”[152] Turing felt that “whether this is possible in principle or not is a stimulating and exciting question, suggested by some of these recent developments but it did not seem that the machines constructed or projected at the time had this property.”[152]”</p>
<p>Turing asked who could be “certain that original work that he has done was not simply the growth of the seed planted in him by teaching, or the e↵ect of following well-known general principles.”[152] “The objection says that a machine can never take us by surprise.”[152]” He found that “machines take me by surprise with great frequency. This is largely because I do not do su cient calculation to decide what to expect them to do, or rather because, although I do a calculation, I do it in a hurried, slipshod fashion, taking risks.”[152]</p>
<p>Reministant of Godel’s theorem Turing again cautioned “the view that machines cannot give rise to surprises is due, I believe, to a fallacy to which philosophers and mathematicians are particularly subject. This is the assumption that as soon as a fact is presented to a mind all consequences of that fact spring into the mind simultaneously with it. It is a very useful assumption under many circumstances, but one too easily forgets that it is false. A natural consequence of doing so is that one then assumes that there is no virtue in the mere working out of consequences from data and general principles.”[152]</p>
<p>Shimon Edelman put it best saying “Turing’s legacy for the cognitive and brain sciences can therefore be summarized by observing that, just as nothing in biology makes sense except in the light of evolution, as Dobzhansky famously remarked, nothing about the mind/brain makes sense except in the light of computation”.[84] Peter Denning suggests “computing is no longer a science of the artificial. It is a science of natural information processes. The remarkable shift to this realization occurred only in the last decade. Computing is mature enough to be described in terms of its fundamental principles. The principles reveal computing’s deep structure and how it applies in many fields. They reveal common aspects of technology and create opportunities for innovation. They open entirely new ways to stimulate the excitement and curiosity of young people about the world of computing. In the 1940s, computation was seen as a tool for solving equations, cracking codes, analyzing data, and managing business processes. By the 1980s, computation had advanced to become a new method in science, joining the traditional theory and experiment. During the 1990s, computation advanced even further as people in many fields discovered they were dealing with information processes buried in their deep structures – for example, quantum waves in physics, DNA in biology, brain patterns in cognitive science, information flows in economic systems. Computation has entered everyday life with new ways to solve problems, new forms of art, music, motion pictures, and commerce, new approaches to learning, and even new slang expressions.”[35]</p>
<p>In 1911 Stephane Leduc laid the foundation for synthetic and artificial biology when he cautioned “the synthesis of life, should it ever occur, will not be the sensational dis- covery which we usually associate with the idea. If we accept the theory of evolution, then the first dawn of the synthesis of life must consist in the production of forms intermediate between the inorganic and the organic world, forms which possess only some of the rudimentary attributes of life, to which other attributes will be slowly added in the course of development by the evolutionary action of the environment.“[88] This would seem to be the primary goal of complex systems and brain sciences, to discover the intermediate forms that mark the emergence of brain like objects.</p>
<p>Andrew Coward, a systems engineer who’s background includes the development and maintenance of large scale real-time electronic telecommunications networks, has sug- gested that the “practical needs of very complex learning systems include resource limitations and learning without interference with prior learning“ and that what is re- quired is to build a “map between the performance of cognitive tasks and the processes at anatomical, physiological and chemical levels that implement the tasks.” Coward claims this can be a way to “understand the performance of cognitive tasks in terms of brain anatomy, physiology and chemistry using techniques analogous with those used in computer science to relate system features to transistor operations“.[31]Further Andrew Coward has laid out a very through outline for understanding the function of brain anatomy in term of a Recommendation System, he describes “the tasks re- quired of a complex, dynamic system include performing many di↵erent behaviours, behavior recommendation and selection, behaviour priority and sequence manage- ment, detecting and defining many di↵erent conditions, heuristically defining most of the conditions, limiting the resources required, condition resource management, and information flow management” [31].</p>
<p>Often called the Common Cortical Algorithm hypothesis there is the idea that a lot of what we consider human intelligence can be explained by a single learning algorithm. In particular most perception (input processing) in the brain may be due to one learning algorithm. Experiments have shown that animals that have visual inputs wired to either their auditory cortex or somatosensory cotex can learn to see.[106]</p>
<p>Demis Hassabis co-founder of DeepMind, the AI start up Google acquired for a half a billion dollars is quoted in 2010 as saying “If you last looked seriously at neuroscience circa 2005 - you are out of date.” Many people have never heard of DARPA, the Defense Advanced Research Projects Agency, the organization responsible for among other things the Internet. (not to be confused with the world wide web invented at CERN) Even fewer people have heard of IARPA, Intelligence Advanced Research Projects Activity who has projects that include “a new generation of machine learning algorithms with human-like performance characteristics by using cortical computing primitives as their basis of operation“</p>
<p>Manager of cognitive computing for IBM Research, Dharmendra S. Modha, com- mented: “neuroanatomists have not found a hopelessly tangled, arbitrarily connected network, completely idiosyncratic to the brain of each individual, but instead a great deal of repeating structure within an individual brain and a great deal of homology across species The astonishing natural reconfigurability gives hope that the core algorithms of neurocomputation are independent of the specific sensory or motor modalities and that much of the observed variation in cortical structure across areas represents a refinement of a canonical circuit; it is indeed this canonical circuit we wish to reverse engineer.”</p>
<p>Bruno Olshausen, who together with David Fields started the modern field of sparse modeling in neuroscience agrees “thats where were going to start to learn about the tricks that biology uses. I think the key is that biology is hiding secrets well,” “We just dont have the right tools to grasp the complexity of whats going on.”[129]</p>
<p>“Invariably the explanatory metaphors of a given era incorporate the devices and spectacles of the day and in perhaps subtler ways they may reflect the propellant social forms and daily texture of life. Theorizing about brain and mind has been es- pecially susceptible to sporadic reformulation in terms of the technological experience of the day. For example, the water technology of antiquity (fountains, pumps, water clocks) underlies the Greek pneumatic concept of the soul (pneuma) and the Roman physician Galen’s theory of the four humours; the clockwork mechanisms proliferat- ing during the Enlightenment are ticking with seminal influence inside Le Mettrie’s L’Homme machine; Victorian pressurized steam engines and hydraulic machines are churning beneath Freud’s hydraulic construction of the unconscious and it’s libidinal economy; the arrival of the telegraph network provided Helmholtz with his basic moral metaphor, as did reverberating relay circuits and solenoid for hemispheric memory and so on.”[34]</p>
<p>“The mind can be understood in terms of the brain ‘The Astonishing Hypothesis’ is that ‘You,’ your joys and your sorrows, your memories and your ambitions, your sense of personal identity and free will, are in fact no more than the behavior of a vast assembly of nerve cells and their associated molecules.” - Francis Crick [33]</p>
<p>Neural networks as models of the human brain can be traced back to Ramon y Cajal’s “neuron doctrine”, Golgi staining, and subsequent 1906 Nobel prize for this work [165], and even further back to Alfred Smee’s work in 1849 [146].</p>
<p>In 1943 Warren S. McCulloch, a neuroscientist, and Walter Pitts, a logician, devel- oped the first generation of neural networks as simplified models of nervous function [105]. Biological neurons are incredibly complex. To fully describe all of the detailed chemistry and molecular operations of even just a single cell is not feasible by even the world’s fastest computers.</p>
<p>Neural networks are an attempt to model the information1 processing properties of a neuron using mathematics. The first generation of neural models were networks of switches with an all-or-nothing on/o↵ characteristic. In other words, the neuron was a two-state device. The mathematics of switching and network theory, popular in the early 20th century, seemed natural tools to model the nervous system. These early networks of switches were able to reproduce simple logical functions, and it was thought that the brain might be a system for implementing logic in switches, similar to the early telephone networks. With the advent of the theory of computation due to Alan Turing, a new science emerged and possibility that the brain could be modeled as a universal digital computer.</p>
<p>The second generation of neural networks describes models with an analog or contin-</p>
<p>uous output response. This allows for more accurate modeling, as well as providing</p>
<p>greater utility in computer applications that rely on neural networks for there op-</p>
<p>eration. Most of the neural networks popular in research today are of this second</p>
<p>generation. The most important development in second generation neural networks</p>
<p>1Given a learning system L trained on a dataset D with error or energy level E the information content of a signal d is proportional to the change in error or energy when L is trained on D + d. Or alternatively: Given a learning system L, trained on a dataset D that results in parameters P, the information content of a signal d is proportional to the change in parameters P when L is trained onD+d. H(d)=L(D) L(D+d)</p>
<p>was the discovery of the backpropagation algorithm. Backpropagation is a technique used to adjust the parameters, otherwise known as weights, in a neural network model. Essentially a form of the chain rule, backpropagation modifies the weights in a neural network until a particular loss function is minimized. For example, we will consider the case of supervised learning in a three-layer neural network. The three layers con- sist of an input layer, a hidden layer, and an output layer. The input layer represents the pixel intensities of a two-dimensional gray-scale image. The hidden layer learns the features that separate the input vectors and the output layer provides the labels.</p>
<p>The third generation of neural networks describes mathematical models in which time plays an active element in the operation and function in the network. This idea was popularized by John Hopfield in the 1980s, who applied statistical models from the physics of spin glasses and Ising models to the problem of mathematical neurons connected in a network. This paved the way for the interpretation of neural networks as a form of natural computing, and that they might ultimately be implemented in non-biological substrates. Neural models that can be implemented in hardware allow for direct emulation rather than digital simulation. There is continuing need for high speed, low power, electronic sensors and computers, and third generation analog neural networks in hardware present a possible solution.</p>
<p>Machine learning is a branch of computer science that deals with algorithms that can adjust their own parameters based on the data given to the algorithm. The classic example is Arthur L. Samuel’s 1959 “Studies in Machine Learning Using the Game of Checkers” where “Enough work has been done to verify the fact that a computer can be programmed so that it will learn to play a better game of checkers than can be played by the person who wrote the program.”[137] We find that these ideas are not uncommon, a patent by Putzrath in 1961 reads “the present invention relates to information processing apparatus, and more particularly to electrical apparatus for recognizing patterns, such as speech patterns, by simulated neural processes.” [122] A few years later Ed Feigenbaum would pronounce “The thing we call ’A.I.’ - computers doing intelligent things - is the manifest destiny of Computer Science.“ [50]</p>
<p>Deep learning is based on two big ideas, learning features from unlabeled data and learning multiple layers of representation. [110] Despite many decades of research, a broad theoretical approach to the brain remains elusive. Behavioral and physiological approaches have often been stymied by the complexity of the living brain. Theoretical and computational approaches, while o↵ering greater experimental flexibility, may be too artificial in that they do not generally take into consideration the constraints and demands of a real agent acting in a physical environment.</p>
<p>“One way of setting about our task of building a thinking machine would be to take a man as a whole and to try to replace all the parts of him by machinery. He would include television cameras, microphones, loudspeakers, wheels and handling servo-mechanisms as well as some sort of electronic brain. This would of course be a tremendous undertaking. The object if produced by present techniques would be of immense size, even if the brain part were stationary and controlled the body from a distance. In order that the machine should have a chance of finding things out for itself it should be allowed to roam the countryside…” [151]</p>
<p>Embedding the models from computational neuroscience into robotic agents that will sense, act and learn in a real-world environmeng using simple, low-cost, wireless robotic devices (rovers), will allow the construction of a unified model of perception and action based on neurologically inspired machine-learning networks.</p>
<p>Remotely Operated Vehicle for Education and Research (R.O.V.E.R.) consisting of a color video camera and a microphone, with the ability to move via tread motion. Each rover is independently controlled wirelessly by a devoted brain, consisting of an artificial neural network housed on a separate computer (local cloud). These computers both control the rover and receive and record the resulting perceptual feedback. The rovers behave within a physical environment and are subjected to reinforcement protocols (i.e. reward and punishment positive and negative feedback in response to a behavioral outcome) similar to those employed in behavioral neuroscience. Learning of both perceptual features as well as action selection is based on neurally inspired machine learning architectures applied over this perception/ac- tion/feedback data stream. The rovers are tested for their ability to learn relatively simple behaviors, such as obstacle avoidance, future work will explore more complex goals such as social interaction between rovers with the overarching goal of developing a unified approach to embedded biological intelligence.</p>
<p>Recent advances in a class of machine learning derived from biological neural systems (deep learning networks) have led to remarkable progress in previously unsolved problems in neuroscience, computer vision and robotics. More recent research has successfully implemented these techniques in reinforcement learning (RL) contexts as well, for example artificial-intelligence (AI) based on machine learning video game play. These development results suggest that machine learning may provide a unified theoretical framework for understanding biological neural systems. However, previous research efforts in these areas has taken place independently in different disciplines, with little attempt to integrate them. The proposed research therefore aims to combine the latest developments in mathematical learning theory and experimental neuroscience to create a testbed for further understanding neurally based complex behaviors.developing and testing theories of embedded biological intelligence. This project aims to establish FAU as a leader in emerging artificial intelligence (AI) technologies and computational neuroscience and build research connections between departments, colleges, and campuses. Current and future areas of multidisciplinary research include autonomous vehicles, environmental monitoring, visual and auditory prosthesis, automated image analysis, and computational medicine.</p>
<p>“The organisation of a machine into a universal machine would be most impressive if the arrangements of interference involve very few inputs. The training of the human child depends largely on a system of rewards and punishments, and this suggests that it ought to be possible to carry through the organising with only two interfering inputs, one for pleasure or reward (R) and the other for pain or punishment (P).” [151]</p>
<p>To demonstrate that a machine-learning architecture can develop purposeful behavior sequences within a real-world environment. The primary goal of this proposal is to develop and test a novel research paradigm: a robotic ‘model organism’ with a flexible, biologically motivated neural architecture whose behavior is determined by a reinforcement learning protocol. The goal is to demonstrate that basic cognitive functions, such as navigation and reward goal seeking, may be realized in these robots using a unified machine-learning architecture, validating both the overall research approach and as well as the hypothesis that machine learning provides a broad unified theoretical framework for understanding biological intelligence.</p>
<p>To develop complex behavioral patterns including delayed reward such as hierarchical goal-seeking, and social interactions and communication.. A second stage of this research will assess the robots ability to engage in social more complex behaviors such as generating sub-goals in service of larger goals as well as social behavior such as modeling the behaviors actions of other robots and competing and/or cooperating with them. The rovers are equipped with microphones and light, allowing for active communications among them.</p>
<p>To assess differences across neural architectures, including pathology.pathology. Once the basic proof-of-concept has been established, this paradigm may be used to compare theoretical behavioral models (e.g. architectures with varying levels of computational power), and to model potential dysfunctions underlying disorders, such as memory and learning deficits by disrupting the relevant simulated neural mechanisms and observing the resulting neural and behavioral consequences.</p>
<p>“A man provided with paper, pencil, and rubber, and subject to strict discipline, is in effect a universal machine.” Alan Turing 1948</p>
<p>Turing wrote that the “idea behind digital computers may be explained by saying that these machines are intended to carry out any operations which could be done by a human computer.”[152] Turing claims that the “analogy with the human brain is used as a guiding principle.” [151] Thomas Insel of NIMH has cautioned that “there is a sense from many places that whoever figures out how the brain computes will come up with the next generation of computers.” Bruno Olshausen has observed that “if you could solve these problems, its going to open up a vast, vast potential of commercial value.”</p>
<p>Turing wished to investigate other types of unorganised machine, and he envisaged
the procedure nowadays used extensively by connectionist of simulating a neural network and its training regimen using an ordinary digital computer (just as an engineer may use a computer to simulate an aircraft wing or a weather analyst to simulate a storm system). [153] The crowd sourced Internet encyclopedia defines ’Connection- ism’ is “a set of approaches in the fields of artificial intelligence, cognitive psychology, cognitive science, neuroscience, and philosophy of mind, that models mental or behavioral phenomena as the emergent processes of interconnected networks of simple units.” However Jack Copeland has pointed out that “it is not widely realized that Turing wrote a blueprint for much of the connectionist project as early as 1948.”[153] Turing knew that the “potentialities of human intelligence can only be realised if suit- able education is provided.”[151] Turing himself suggested that “the training process renders certain neural pathways effective and others ineffective.”[153] I.J. Good later explained how “the machine will be able to learn from experience, by means of positive and negative reinforcement, and the instruction of the machine will resemble that of a child.” [62] Copeland suggests that “from a historical point of view, Turing’s idea that an initially unorganized neural network can be organized by means of interfering training is of considerable significance, since it did not appear in the earlier work of McCulloch and Pitts.”[153] Copeland reports that “so far as is known, [Turing] was the first person to consider building computing machines out of trainable networks of randomly arranged neuron-like elements.”[153]
“Compressed sensing and sparse methods have played an important role in the medi- cal imaging field, including image reconstruction, image enhancement, image segmentation, anomaly detection, disease classification, and image database retrieval.”[49]</p>
<p>The CS theory builds upon the fundamental fact that many signals can be represented using only few (sparse), linearly combined, elements of a suitable basis or dictionary.[119] Sparsity is the crucial property in the CS framework, as without sparse representation in the higher dimensional space, the lower dimension random projections are not su cient for e↵ective reconstruction.[119] The theory of com- pressed sensing originates from results in the field of high-dimensional statistics.[119] The oldest of these algorithms derives from combinatorial group testing during WWII. In these problems we suppose that there are n total items and k anomalous elements that we are seeking.[47] By a sparse representation, we mean that for a signal of length n, we can represent it with k less than n nonzero coefficients; by a compressible representation, we mean that the signal is well-approximated by a signal with only k nonzero coefficients.[47] Sparse approximation “forms the foundation of transform coding schemes that exploit signal sparsity and compressibility, including the JPEG, MPEG, and MP3 standards.”[47] According to Candes the “crucial observation is that one can design e cient sensing or sampling protocols that capture the useful information content embedded in a sparse signal and condense it into a small amount of data.” [24] Nonlinear optimization algorithms “lead to recovery of signals from very few measurements, significantly fewer than required by the Shannon- Nyquist sampling theorem.”[119] These results “can be of material value for multiple facets of neuroscience research, particularly for neuronal data analysis, fluorescence microscopy, gene-expression analysis, and connectomics.”[119] The “groundbreaking contribution” of compressed sensing is that a “simple, linear measurement process” can allow for a compressed encoding and e cient decoding.[119] A neural network can be considered “as the projection from one brain area to another via a convergent axonal pathway.”[119] While it might seem that a recovery task would be “impossible because there is no way to reconstruct a signal during the times/places that the signal is not measured,”[25] CS has however shown that this “seemingly impossible optimization program (subset selection) can be solved using a tractable amount of computation.”[41]</p>
<p>Signals that are compressible and sparse can be “represented with high fidelity by preserving only the values and locations of the largest coe cients of the signal.” [47] These protocols are nonadaptive and simply require correlating the signal with a small number of fixed waveforms that are incoherent with the sparsifying basis. [24] Incoherency means that any column of the sensing matrix has dense (opposite of sparse) representation in the matrix (i.e write one column as a linear combination of other columns and the coe cients will be dense) [119] Candes describes incoherence as a phenomena that “extends the duality between time and frequency; just as a Dirac (spike) in the time domain is spread out in the frequency domain, incoherence expresses the idea that objects having a sparse representation must be spread out in the domain in which they are acquired.”[24] When considering a neural network model “the basis set can be represented by the activity of cells that exhibit certain properties, regarding, e.g., their receptive fields, such as mammalian visual cortex cells , or their spatial firing patterns, such as grid cells.” [119] Thus we see have that the “representation of a signal is actually ‘summarized’ by the encoded, compressed version through a measurement/sensing procedure.”[119] We can now answer “whether neural circuits are capable of implementing L1 minimization”[119], we can see that LCA is a simple model for how this behavior might occur in biological neural networks.</p>
<p>Aaronson describes an attempt as to how one might simulate a brain, i.e. pass a Turing Test, “maybe we do know all the inputs well ever need, but we just can’t write them in a big enough table, so we write them down in this compressed form.” [1] The major idea of sparse modeling is that the information rate of a signal may be smaller than suggested by traditional signal processing assumptions.[24] For many discrete-time signals the number of degrees of freedom is much smaller than its signal length. [24] Around 2004, Emmanuel Cands, Terence Tao, and David Donoho showed that sparse signals may be “reconstructed with even fewer samples than the sampling theorem requires.”[25] Even though compressed sensing is relatively new topic “thousands of papers have appeared in this area, and hundreds of conferences, workshops, and special sessions have been dedicated to this growing research field.” [47] A typical task in signal processing is to reconstruct a signal from a series of sample measurements.[25] Random sampling protocols allow a sensor to “very efficiently capture the information in a sparse signal without trying to comprehend that signal.” [24] Baraniuk reports “that random projections have recently emerged as a surprisingly useful tool in signal processing.”[12] Work on Random Projections (RP) has led to a “powerful, yet extremely simple methodology for dealing with the curse of dimensionality.”[119] Random projections are a universal in that this “encoding process can proceed without knowledge of the structure that makes the signal compressible.”[12] According to Donoho, we are enabled to “sample smarter not faster; we can replace front-end acquisition complexity with back-end computing”.[41]</p>
<p>Elder reports that although there are extraordinary advances in computer hardware, “the acquisition and processing of signals in application areas such as imaging, video, medical imaging, remote surveillance, spectroscopy, and genomic data analysis continue to pose a tremendous challenge.”[47] As a typical case “ we might wish to identify defective products in an industrial setting, or identify a subset of diseased tissue samples in a medical context.”[47] However we find that “in many important and emerging applications, the Nyquist rate is so high that we end up with far too many samples.”[42] The key point being that the sampling theorem results provides a su cient condition not a necessary one. [25] Baraniuk claims the “key revelation is that the relevant structure in a signal can be preserved when that signal is projected onto a small number of random basis functions.”[12] Now the problem is the “design a collection of tests that allow us to identify the support (and possibly the values of the nonzeros) of x while also minimizing the number of tests performed.”[47]</p>
<p>“Why Random Projections?” asks Durrant, who reflects that random projections are, ”linear, cheap, and universal. Target dimension does not depend on data dimension- ality for Johnson Lindenstrauss Lemma (JLL), and JLL works with high probability for any fixed finite point set. The technique is oblivious to data distribution and tractable to analysis.”[43] We do find that while “information is lost through such a projection, that information tends to be incoherent with the relevant structure in the signal.”[12] These new results “dramatically reduces the number of measure- ments needed for e cient reconstruction, compared with the ones indicated by the Shannon-Nyquist sampling theorem.”[119] We find many diverse motivations for RP in the literature including “avoid the collection of lots of data” and of primary interest here in the “theory of cognitive learning (RP Perceptron)”.[43] In the case of biolog- ical neural network connections “multiplication can be thought of as the influence of one region to another…this could represent projection from the cortex…in this case y would be the activity (firing rates) of a subset of M neurons.[119] Of primary concern to psychologists is the question of “how does the brain learn concepts from a handful of examples when each example contains many features?” [119] Random projections preserve “the concept” and in the new “low-dimensional space, the number of ex- amples and time required to learn concepts are comparatively small.”[43] Neurons can form a basis set “if their (appropriate) combination can generate any signal f (activity pattern of neurons) in the cortex.”[119]</p>
<p>There exists an intimate linkage between the CS theory and the JLL.[11] “With high probability”, it can be shown that “a random projection of a sparse, high- dimensional signal vector onto a lower-dimensional space contains enough information to enable signal reconstruction with small or zero error.”[11] Thus the JLL “shows that with high probability the geometry of a point cloud is not disturbed by certain mappings onto a space of dimension logarithmic in the number of points.” [11] The statement and proofs of the JLL have been simplified considerably by using random linear projections and concentration inequalities. From JLL we have “high-probability guarantees that for a suitably large k, independently of the data dimension, random projection approximately preserves data geometry of a finite point set. In particular norms and dot products approximately preserved with high probability.”[43]</p>
<p>The pixels of a CT scan or of an ‘Ansel Adams’ photograph can be represented as vector f, with N-dimensions and gray scale intensity.[119] In both image examples we find that in practice with very high probability “very few coefficients are needed to represent the image via a wavelet basis set, thus the x vector is sparse.”[119] Typically digital cameras would need take a measurement for each of the N dimensions or pixels, in modern consumer digital cameras that are sensitive in the visible spectrum this is accomplished by using an array of semiconductor detectors. This technology has ridden the wave of Moore’s law and thus we have low cost camera pixel arrays (cell phone camera hardware about $ 1), for light outside the visible spectrum new methods are needed as high resolutions detectors are not possible. Different detector that might be used “include a photomultiplier tube or an avalanche photodiode for low-light (photon-limited) imaging (more on this below), a sandwich of several photodiodes sensitive to different light wavelengths for multimodal sensing, and a spectrometer for hyperspectral imaging.”[42] Many have now suggested we “work with random projections of the data.”[43]</p>
<p>New types of camera architectures have been proposed in which “rather than measuring pixel samples of the scene under view, we measure inner products between the scene and a set of test functions.”[42] This new imaging architecture is based on “a digital micro-mirror device (DMD) with the new mathematical theory and algorithms of compressive sampling (CS).”[42] The DMD single-pixel camera is an Optical Computer (OC) that “measures the inner products between an N-pixel sampled version x of the incident light-field from the scene.” [119] A DMD, consisting of an array of N tiny mirrors, catches the light of a scene; “the reflected light is then collected and focused onto a single photon detector (the single pixel) that integrates the product to compute the measurement as its output voltage, the voltage across the photo detector is then sent to analog to digital converter.[42] To compute CS randomized measurements, we set the mirror orientations randomly using a pseudo- random number generator, measure, and then repeat the process M times to obtain the measurement vector y.” [42] Random test functions play a key role as each measurement is a random sum of pixel values taken across the entire image, and in this manner sub-Nyquist image acquisition is achieved. [42]</p>
<p>Eldar reminds us that a “popular techniques for signal compression is known as transform coding”, and this technique involves “finding a basis or frame that pro- vides sparse or compressible representations for signals in a class of interest.”[47] In Magnetic Resonance Image (MRI) reconstruction, “sparsity in transformed space such as wavelet has been successfully used to speed up scanning time and improve reconstruction quality.”[49] Consider sparsity in the case of a Fourier basis set, “spar- sity implies that the majority of the energy of signal f is contained in a few frequency components.”[119] Duarte discusses how we can combine “sampling and compression into a single non-adaptive linear measurement process.”[42] Donoho, Romberg, Can- des, Tao, Baraniuk and others work has shown that for an “e cient and reversible encoding process” matrices “must fulfill three very important conditions termed spar- sity, incoherency, and isometry.”[119]</p>
<p>Candes describes Compressed Sensing as, “a way to use numerical optimization to reconstruct the full-length signal from a small amount of collected data.” [24] The goal of single-pixel camera design is that it “reduces the required size, complexity, and cost of the photon detector array down to a single unit, which enables the use of exotic detectors.”[42] The Johnson-Lindenstrauss Lemma demonstrates that random projections preserve image structure by embedding points with minimal disruption of their pair-wise distances.[12] When the original image is projected from its N- dimensional space to an M-dimensional space, it has a new compressed version y.[119] Candes describes Compressed Sensing as “a very simple and e cient signal acquisition protocol, which samples in a signal independent fashion at a low rate and later uses computational power for reconstruction from what appears to be an incomplete set of measurements.”[24] When the image is “compressible by an algorithm like JPEG, the CS theory enables us to stably reconstruct an image of the scene from fewer measurements than the number of reconstructed pixels.”[42]</p>
<p>With “prior knowledge or assumptions about the signal”, namely that the signal of interest is sparse in some basis, it turns out to be “possible to perfectly reconstruct a signal from a series of measurements.”[25] Random sampling has been shown to be a good approach for most known basis for example sines, cosines, wavelets, curvelets, etc.[95]Low dimensional RPs form “an e cient encoding that can be used as a com- pressed representation of the original data; high dimensional patterns can then be recovered by appropriate decoding processes.”[119] Compressed sensing theory then tells us that “data with high dimensionality is represented by sparse components of a suitable basis set, it is possible to reconstruct them by their RPs.” [119] Richard Baraniuk and others have suggested that random projections “provide dimensionality reduction, which can significantly simplify certain computations.”[12]</p>
<p>Derived from “ the way the brain processes information, neuroscience, neural net- work, and dynamical systems communities have been proposing novel computational concepts. These concepts are fundamentally di↵erent from the standard Turing or von Neumann Machine methods, which are widely implemented in most computa- tional systems.”[85] Several new algorithms, though discovered independently, share common features and carry “many ideas towards a new computational paradigm of neural networks.”[118] Turing himself had begun to “investigate other types of unor- ganized machines, and also to try out organizing methods that would be more nearly analogous to ‘methods of education’.” [151] Reservoir computing (RC) “recently ap- peared as a generic name for designing a new research stream including mainly echo state networks (ESNs), liquid state machines (LSMs), and a few other models like back-propagation decorrelation (BPDC).” [118] The key component of a RC is “a large, distributed, nonlinear recurrent network, the so-called reservoir, with trainable output connections, devoted to reading out the internal states induced in the reservoir by input patterns.”[118]</p>
<p>The main advantage of RC is to use a “fixed randomly connected network as reser- voir”, without the need of training. [118] One of these concepts is known as Echo State Network, Liquid State Machine or more generally as Reservoir Computing (RC). Larger has described RC as being based on “the computational power of complex recurrent networks operating in a dynamical and transient-like fashion.” Further Larger comments on how “RC benefits from the advantages of recurrent neural net- works, while at the same time avoiding the problems in the training procedure.”[85] Paugam reports that Reservoir Computing is a “family of versatile basic computa- tional metaphors with a clear biological footing.[118] Reservoir Computing is the idea “do not adapt the internal connection weights, initialize them randomly” and only “train the output directly, using a specific classifier or regression method.”[107] In most RC “models linear regression or recursive least mean squares, are applied to readout neurons only.”[118] When the activation function fa out is a linear classifier the Reservoir Computer is called an Echo State Network.[107] Liquid State Machines LSM are “rather similar to the ESN, except for the fact that the function fin is usually a spiking neural network or a network of threshold logic gates.”[107]</p>
<p>The Single-Layer Feedforward Neural Networks architecture known as Extreme Learn- ing Machine (ELM) was proposed by Huang et al.[107] Given that it is feed forward, unlike the Reservoir Computing algorithms, there is “no recurrence in the neural net- work of ELM-based techniques.”[107] The central notion of the ELM is the random nature of the network weights.[107] The only parameter is the number of neurons in the hidden layer and even then “one does not have to know beforehand the exact best number of neurons required for ELM to perform well.”[107] The output weights “can be computed from the hidden layer output matrix H and target values by using a Moore-Penrose generalized inverse of H”[107] It has be shown that “computational time is minimal while adding new random neurons to the ELM.”[107]</p>
<p>Bruno Olshausen has suggested that if we could figure out how biology naturally deals with noisy computing elements, it would lead to a completely di↵erent model of computation. [115] While Geo↵rey Hinton reminds us that “the brain is confronted by a buzzing, blooming confusion. It needs to fit many di↵erent models and use the wisdom of crowds.” [87] Andrew Coward has strong evidence that “unambigu- ous behavioural meanings for condition detections in a complex learning system” are not practical, because they can cause “major interference between new learning and prior learning.” [31] Further Andrew Coward has described how “partially ambiguous behavioural meanings require more information handling resources, but makes it fea- sible to limit interference between new and prior learning.” [31] Andrew claims this is would also be useful to an evolutionary process in that “if information processes are recommendations, there is a lower probability that a mutation will have fatal consequences, because any behaviour must be supported by recommendations from multiple sources.” [31]</p>
<p>Biological systems and modern computing hardware systems are both constrained by size, power, and reliability, thus “unconventional computing methods that directly address these issues are of increasing interest.”[3] There is a clear “need to better understand, and perhaps exploit, probability in computation.”[3] Stochastic com- puting was proposed “as a low-cost alternative to conventional binary computing.”[3] Stochastic computers store and manipulate “information in the form of digitized prob- abilities.” [3] Stochastic computers represents “continuous values by streams of ran- dom bits.”[3] Complex computations are performed with “simple bit-wise operations on the streams.”[3] “Stochastic computing is distinct from the study of randomized algorithms.”[3]</p>
<p>A basic feature of Stochastic Computing numbers themselves are interpreted as prob- abilities in that numbers are represented by streams of bits. The bits can be streams in time on a single wire or multiple wires grouped in space, as with the Bundle Com- puter or both in the case of the Ergodic Computer) that can be “processed by very simple circuits”[3] Stochastic computers are fail-soft in that the probabilistic nature of the elements makes them insensitive to the particular value of any unit, thus pro- viding utility “under both normal and faulty conditions.” [3] “Bit-streams of this type and the probabilities they represent are known as stochastic numbers. For ex- ample, the probability of observing a 1 at an arbitrary bit position in a bit-stream S containing 25% 1s and 75% 0s is p = 0.25. Neither the length nor the structure of S need be fixed, and the positions can, in principle, be chosen randomly.”[3] Stochas- tic computing enables very low-cost implementations of arithmetic operations using standard logic elements. Multiplication can be performed in a stochastic circuit by a single AND gate, “consider two binary bit-streams that are combined with logical AND. If the probabilities of seeing a 1 on the input bit-streams are p1 and p2, then the probability of 1 at the output of the AND gate is p1 p2, assuming that the two bit-streams are suitably uncorrelated or independent.”[3]</p>
<div class="toctree-wrapper compound">
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='right-next' id="next-link" href="functions.html" title="next page"><span class="section-number">1. </span>Functions</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By William Edward Hahn<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>